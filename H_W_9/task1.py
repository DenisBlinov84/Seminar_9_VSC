# Даны значения величины заработной платы заемщиков банка (zp)
# и значения их поведенческого кредитного скоринга (ks):
# zp = [35, 45, 190, 200, 40, 70, 54, 150, 120, 110], ks = [401, 574, 874, 919, 459, 739, 653, 902, 746, 832].
# Используя математические операции, посчитать коэффициенты линейной регрессии,
# приняв за X заработную плату (то есть, zp - признак),
# а за y - значения скорингового балла (то есть, ks - целевая переменная).
# Произвести расчет как с использованием intercept, так и без.

# Посчитать коэффициент линейной регрессии при заработной плате (zp), используя градиентный спуск (без intercept).

# Произвести вычисления как в пункте 2, но с вычислением intercept.
# Учесть, что изменение коэффициентов должно производиться на каждом шаге одновременно
# (то есть изменение одного коэффициента не должно влиять на изменение другого во время одной итерации).

from sklearn.linear_model import LinearRegression
import scipy.stats as stats
import numpy as np
import matplotlib.pyplot as plt

zp = np.array([35, 45, 190, 200, 40, 70, 54, 150, 120, 110])
ks = np.array([401, 574, 874, 919, 459, 739, 653, 902, 746, 832])
n = 10

# Проведём тест Шапиро-Уилка для определения нормальности распределения остатков:
resid = zp-ks
print(stats.shapiro(resid))
# ShapiroResult(statistic=0.8939908742904663, pvalue=0.18797828257083893)
# т.к. pvalue > альфа, то различий с нулевой гипотезой не обнаружено, распределение остатков следует нормальному распределению

# Расчёт коэффициентов по формулам:
b1 = (np.mean(zp*ks)-np.mean(zp)*np.mean(ks))/(np.mean(zp**2)-np.mean(zp)**2)
b0 = np.mean(ks)-b1*np.mean(zp)
y_pred = b0+b1*zp  # оценочные значения
print(b1, b0)

# Функция потерь:
mse = ((ks-y_pred)**2).sum()/n


# Матричный метод расчёта коэффициентов линейной регрессии:
x = zp.reshape((10, 1))
y = ks.reshape((10, 1))
X = np.hstack([np.ones((10, 1)), x])
B = np.dot(np.linalg.inv(np.dot(X.T, X)), X.T@y)
print(B)


# Расчёт коэффициентов методом градиентного спуска, без интерсепта (y=b1*x) :
B1 = 0.1
alpha = 1e-6


def mse_(B1, ks=ks, zp=zp, n=10):
    return np.sum((B1*zp-ks)**2)/n


# mse=1/n*np.su,((B1*zp-ks)**2)
mse = (2/n)*np.sum((B1*zp-ks)*zp)

for i in range(3000):
    B1 -= alpha*(2/n)*np.sum((B1*zp-ks)*zp)
    if i % 500 == 0:
        print('Iteranion = {i}, B1={B1},mse={mse}'.format(
            i=i, B1=B1, mse=mse_(B1)))

# посчитаем mse через записанную ранее функцию и убедимся, что они одинаковы:
print(mse_(5.889815595583751))


# Функция в Python для построения линейной регрессии:
model = LinearRegression()  # зададим модель линейной регрессии
# делаем массив s двумерным атрибутом reshape(-1,1)
s = zp.reshape(-1, 1)
regres = model.fit(s, ks)  # подбираем коэффициенты
print(regres.intercept_)  # выводим интерсепт
print(regres.coef_)  # выводим коэффициент



